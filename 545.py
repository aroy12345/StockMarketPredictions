# -*- coding: utf-8 -*-
"""Copy of Copy of 545_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fO-46OiKik7AbfBwxJS5-uOkPjcRLJwS

<font size = "64"> **CIS 5450 Group Project** </font>

---

#Tokyo Stock Market Expected Returns Prediction

###*Divek Patel, Anirudh Bharadwaj, Aryan Roy*

## **Background**
Predicting stock returns through quantitative analysis is a crucial challenge as well as the largest opportunity in the fields of finance and investment. This is because it not only provides methods for model trading programmatically, but generally provides investors with insight on identifying potentially profitable investment opportunities, optimizing their clients’ portfolios and enabling effective risk management worthy of their reputations. Thus, as a necessary tool for ensuring reliability in increasingly volatile markets, many financial institutions have recently turned toward data-driven predictive algorithms and processes to provide more accurate and calculative decisions to determine securities with the largest returns.

One company in the pursuit of such predictive models is the Japan Exchange Group, Inc. (JPX), one of the largest companies that operates the Tokyo Stock Exchange. Their released data, which we will be using for this project, consists of historical time-series stock trading data in the Japanese market from 2017-2020. This will be especially interesting, as foreign market data is usually restricted for public use, and finding these comprehensive datasets presents an opportunity to inquire further into international markets.

## **Data Description**

The objective of this project is to predict the future returns of each of the stocks, where the target evaluation metric is the rate of change between the closing price two days from the trade date ($C_{k+2}$) and the closing price one day from the trade date  ($C_{k+1}$). In other words, it is the change ratio of adjusted closing price between days $t+2$ and $t+1$,  where day $t+0$ is the date of confirmed trade.

We define the target change ratio $r_c$ as follows: $$r_c = \frac{(C_{k+2} - C_{k+1})}{C_{k+1}}$$.

The core file of interest and analysis is “stock_prices.csv”, which contains stock data, such as Open, High, Low, Close, Volume, etc., on the 2,000 most commonly traded equities in Japan’s markets. For additional analysis and comparison, “secondary_stock_prices.csv” records less liquid securities, but is useful in determining a more general distribution of Japanese markets. Additionally, “stock_list.csv” provides additional qualitative information on each stock, specifically details its market sector (17 & 33 sector) and sections (Prime Market, Standard Market, JASDAQ, ETFs, etc.).

We will first conduct Exploratory Data Analysis and Feature Selection, followed by Model Selection/Training and interpretation of results.

## **PART 1: Exploratory Data Analysis**

### 1.0: Import Libraries

We will first import all necessary packages for the EDA, as well as connecting to Google Drive for data extraction.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# importing data from drive
from google.colab import drive
drive.mount('/content/drive')

"""### 1.1: Instantiate & Prepare Datasets

The goal in this step is to import the datasets and initially clean the data before subsequent EDA.

From Google Drive, we will bring all the needed datasets, such as stock_prices.csv (set to main_stocks_df) and stock_list.csv (set to stock_list_df), to the project notebook.
"""

#Instantiate datasets from Google Drive
main_stocks_df = pd.read_csv("drive/MyDrive/stock_prices.csv")
stock_list_df = pd.read_csv('drive/MyDrive/stock_list.csv')

"""Let's look at the layout of main_stocks_df and stock_list_df."""

#Prints the shape layout of main_stocks_df
print("main_stocks_df shape:", main_stocks_df.shape)
main_stocks_df.info()

#Determines the number of empty entries in every column in main_stocks_df
main_stocks_nan_count = main_stocks_df.isna().sum()
print(main_stocks_nan_count)

#Prints the shape layout of stock_list_df
print("stock_list_df shape:", stock_list_df.shape)
stock_list_df.info()

"""Clearly, the 'ExpectedDividend' column in main_stocks_df is almost entirely empty, so we will drop this column from the dataset. Since there are only a few entries that have NaN values besides 'ExpectedDividend', we will drop these rows."""

#Drop 'ExpectedDividend' column from main_stocks_df
main_stocks_df = main_stocks_df.drop(columns = ['ExpectedDividend'])

#Drop remaining null rows from main_stocks_df
main_stocks_df = main_stocks_df.dropna()

# Check new shape of main_stocks_df
main_stocks_df.shape

"""Finally, we will merge main_stocks_df with stock_list_df through 'SecuritiesCode' to obtain all the qualitative data to each stock. This merged DataFrame will be main_stocks_with_names."""

#Merge main_stocks_df with stock_list_df on 'SecuritiesCode'
main_stocks_with_names = main_stocks_df.merge(stock_list_df,on='SecuritiesCode')

main_stocks_with_names

"""###1.2: Data Visualization"""

market_cap_grouped = stock_list_df.groupby(by='17SectorName').sum().reset_index()[['17SectorName', 'MarketCapitalization']]
market_cap_grouped = market_cap_grouped[market_cap_grouped['MarketCapitalization'] > 0].sort_values(by='MarketCapitalization',ascending=False).reset_index()
market_cap_grouped['17SectorName'] = market_cap_grouped['17SectorName'].apply(lambda x: "Other" if x == "-" else x)

# Attribution : https://stackoverflow.com/questions/7082345/how-to-set-the-labels-size-on-a-pie-chart-in-python

labels = market_cap_grouped['17SectorName']
sizes = market_cap_grouped['MarketCapitalization']


explode = (0.2,0.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
plt.figure(figsize=(30,30))
fig, ax = plt.subplots()
ax.pie(sizes, explode=explode, labels=labels, shadow=True, textprops ={'fontsize' : 5})
plt.show()

"""We see that IT & Services and Electric Appliances are by far the two largest sectors when considering the cumulative market cap."""

market_cap_grouped_33 = stock_list_df.groupby(by='33SectorName').sum().reset_index()[['33SectorName', 'MarketCapitalization']]
market_cap_grouped_33 = market_cap_grouped_33[market_cap_grouped_33['MarketCapitalization'] > 0].sort_values(by='MarketCapitalization',ascending=False).reset_index()
market_cap_grouped_33['33SectorName'] = market_cap_grouped_33['33SectorName'].apply(lambda x: "Other" if x == "-" else x)

#plt.figure(figsize=(6,6))
#ax = sns.barplot(data=market_cap_grouped_33, x='33SectorName', y='MarketCapitalization')
#plt.xticks(rotation=90)
#plt.title('Sectors by Market Capitalization')
#plt.show()

"""**Largest Companies**"""

largest_companies = stock_list_df.sort_values(by='MarketCapitalization',ascending=False)
largest_companies = largest_companies[largest_companies['Section/Products'] != 'ETFs/ ETNs']
largest_companies = largest_companies.head(25)

plt.figure(figsize=(8,8))
ax = sns.barplot(data=largest_companies, x='Name', y='MarketCapitalization')
plt.xticks(rotation=90)
plt.title('25 Largest Companies')
plt.show()

largest_sector_companies = stock_list_df[stock_list_df['Section/Products'] != 'ETFs/ ETNs']
largest_sector_companies =  largest_sector_companies.groupby(by='33SectorName').max().reset_index()[['33SectorName', 'MarketCapitalization']]
largest_sector_companies = largest_sector_companies.sort_values(by='MarketCapitalization',ascending=False)
largest_sector_companies = largest_sector_companies.merge(stock_list_df[['Name', 'MarketCapitalization']], how='left', on='MarketCapitalization')

plt.figure(figsize=(8,8))
ax = sns.barplot(data=largest_sector_companies, x='Name', y='MarketCapitalization')
plt.xticks(rotation=90)
plt.title('Largest Company By Sector')
plt.show()

"""**Overall size of Companies by Sector**"""

sorted_list_df = stock_list_df[stock_list_df['MarketCapitalization'] > 1000000000000]
sorted_list_df = sorted_list_df[stock_list_df['17SectorName'] != '-']

plt.figure(figsize=(10,10))
ax = sns.boxplot(data=sorted_list_df, x='MarketCapitalization', y='17SectorName')
plt.xticks(rotation=90)
plt.title('Size of Companies by Sector')
plt.show()

main_stocks_with_names

earliest_dates = main_stocks_with_names[['Name','Date']].groupby(by='Name').min().reset_index() # earliest dates with data
latest_dates = main_stocks_with_names[['Name','Date']].groupby(by='Name').max().reset_index() # latest dates with data

opening_prices = earliest_dates.merge(main_stocks_with_names,how='left',on=['Name','Date'])[['Name','Date','Open']].dropna()
closing_prices = latest_dates.merge(main_stocks_with_names,how='left',on=['Name','Date'])[['Name','Date','Close_x']].dropna()

prices = opening_prices.merge(closing_prices,how='inner',on='Name')[['Name','Open','Close_x']]

prices['% Change'] = 100 * (prices['Close_x'] - prices['Open']) / prices['Open']

best_performers = prices.sort_values(by='% Change',ascending=False).head(50)
worst_performers = prices.sort_values(by='% Change').head(50)

best_performers

plt.figure(figsize=(8,8))
ax = sns.barplot(data=best_performers, x='Name', y='% Change')
plt.xticks(rotation=90)
plt.title('Best Performing Companies')
plt.show()

plt.figure(figsize=(8,8))
ax = sns.barplot(data=worst_performers, x='Name', y='% Change')
plt.xticks(rotation=90)
plt.title('Worst Performing Companies')
plt.show()

"""Of the ~2000 companies we have data for, we will now consider the best and worst performers among the 100 largest companies (which we define as the 100 largest companies by market capitalization, using the latest market capitalization figures we have).

We will first obtain the most recent market capitalization data we have for each company.
"""

market_caps = latest_dates.merge(main_stocks_with_names,how='left',on=['Name','Date'])[['Name','Date','MarketCapitalization']].dropna()
largest_companies = market_caps.sort_values(by='MarketCapitalization',ascending=False).head(100)

performance = largest_companies.merge(prices,on='Name',how='left')[['Name','% Change']]

best_large_performers = performance.sort_values(by='% Change',ascending=False).head(20)
worst_large_performers = performance.sort_values(by='% Change').head(20)

plt.figure(figsize=(8,8))
ax = sns.barplot(data=best_large_performers, x='Name', y='% Change')
plt.xticks(rotation=90)
plt.title('Best Performing Large Companies')
plt.show()

plt.figure(figsize=(8,8))
ax = sns.barplot(data=worst_large_performers, x='Name', y='% Change')
plt.xticks(rotation=90)
plt.title('Worst Performing Large Companies')
plt.show()

"""From the graphs above, it seems that we observe lower volatility when only considering the largest companies.

We'll now explore if our hunch is right by determining the correlation between a company's market capitalization and the magnitude of the percent change observed.
"""

corr_df = prices.merge(market_caps,how='inner',on='Name')
corr_df['Absolute % Change'] = corr_df['% Change'].apply(lambda x : abs(x))
corr = corr_df[['Absolute % Change','MarketCapitalization']]
corr_mat = corr.corr()

plt.figure(figsize=(8,8))
ax = sns.heatmap(corr_mat,annot=True)
plt.show()

"""It seems (somewhat counterintuitively) that there is very little correlation between market capitlization and the absolute % change (at least in the data we have)."""

opening_market_cap = earliest_dates.merge(main_stocks_with_names,how='left',on=['Name','Date'])[['Name','17SectorName','Date','MarketCapitalization']].dropna()
closing_market_cap = latest_dates.merge(main_stocks_with_names,how='left',on=['Name','Date'])[['Name','17SectorName','Date','MarketCapitalization']].dropna()

market_caps = opening_market_cap.merge(closing_market_cap,how='inner',on='Name')[['Name','17SectorName_x','MarketCapitalization_x','MarketCapitalization_y']]

market_caps['% Change'] = 100 * (market_caps['MarketCapitalization_y'] - market_caps['MarketCapitalization_x']) / market_caps['MarketCapitalization_x']

market_caps

main_stocks_with_names

"""##**Part 2: Modelling**
We'll now attempt to develop a model to predict our target value: the target change ratio $r_c$, which we defined earlier as follows: $r_c = \frac{(C_{k+2} - C_{k+1})}{C_{k+1}}$.

### Data Preprocessing
"""

mn = main_stocks_with_names.groupby("SecuritiesCode").mean()
target = mn['Target']
features = mn.drop('Target',axis=1)

"""We will first split our data into training data and validation data (in order to test how well any model we train on the training data does), setting aside 25% of the data to ensure there is no overfitting.

"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(features,target,test_size = 0.25, random_state = 42)

"""#### Feature Selection
We begin the process by first determining the most relevant features in our data, which we will do using Principle Component Analysis (PCA).

#####Imports
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

"""##### PCA"""

scaler = StandardScaler()
scaler.fit(X_train)
scaled_X_train = scaler.transform(X_train)
scaled_X_test = scaler.transform(X_test)

pca = PCA(n_components = 7)
pca_X_train = pca.fit_transform(scaled_X_train)
pca_X_test = pca.transform(scaled_X_test)

explained_variance_ratios = pca.explained_variance_ratio_
components = np.cumsum(pca.explained_variance_ratio_)

new_pca = PCA(n_components=1)
pca_X_train = new_pca.fit_transform(scaled_X_train)
X_test_pca = new_pca.transform(scaled_X_test)

#y_train = y_train.astype('int64')

"""##Model Selection

We will now try a variety of models and determine their accuracy on our validation data (y_test). We will broadly split our models into two classes (Regressors and Neural Networks).

### Regressors

####HistGradientBoostingRegressor
"""

from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

h_rmse_train_accuracies = []
h_rmse_test_accuracies = []
h_r2_train_accuracies = []
h_r2_test_accuracies = []
for i in range(1,101):
  clf = HistGradientBoostingRegressor(random_state = 0,max_iter=i).fit(X_train,y_train)

  y_train_pred = clf.predict(X_train)
  y_test_pred = clf.predict(X_test)

  score_train = mean_squared_error(y_train_pred,y_train,squared=False)
  score_train = score_train / (y_train.max() - y_train.min())

  score_test = mean_squared_error(y_test_pred,y_test,squared=False)
  score_test = score_test / (y_test.max() - y_test.min())

  h_rmse_train_accuracies.append(score_train)
  h_rmse_test_accuracies.append(score_test)

  r2_train = r2_score(y_train,y_train_pred)
  r2_test = r2_score(y_test,y_test_pred)

  h_r2_train_accuracies.append(r2_train)
  h_r2_test_accuracies.append(r2_test)

plt.figure(figsize=(4,4))
ax1 = sns.lineplot(data=h_rmse_train_accuracies)
ax2 = sns.lineplot(data=h_rmse_test_accuracies)
plt.xticks()
plt.title('RMSE as Iterations Increase')
plt.show()

plt.figure(figsize=(4,4))
ax1 = sns.lineplot(data=h_r2_train_accuracies)
ax2 = sns.lineplot(data=h_r2_test_accuracies)
plt.xticks(range(1,51))
plt.title('R2 as Iterations Increase')
plt.show()

"""We see that the HistGradientBoostingRegressor

####LinearRegression
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import median_absolute_error
from sklearn.metrics import r2_score

reg = LinearRegression().fit(X_train, y_train)
y_pred = reg.predict(X_test)
lin_reg_score = mean_squared_error(y_test,y_pred,squared=False)
lin_reg_score = lin_reg_score / (y_test.max() - y_test.min())
mae = mean_absolute_error(y_test,y_pred)
made = median_absolute_error(y_test, y_pred)
r2 = r2_score(y_test,y_pred)

print(r2)



"""####LinearRegression (w/ PCA)"""

plt.figure(figsize=(8,8))
plt.plot(np.arange(1,8),components)
plt.title('Proportion of Explained Variance as Number of Components Increases')
plt.xlabel('Number of Components')
plt.ylabel('Proportion of Explained Variance')
plt.show()

reg = LinearRegression().fit(pca_X_train, y_train)
y_pred = reg.predict(pca_X_test)
lin_reg_score = mean_squared_error(y_test,y_pred,squared=False)
lin_reg_score = lin_reg_score / (y_test.max() - y_test.min())
r2 = r2_score(y_test,y_pred)

"""####RandomForestRegressor"""

from sklearn.ensemble import RandomForestRegressor

r_rmse_train_accuracies = []
r_rmse_test_accuracies = []
r_r2_train_accuracies = []
r_r2_test_accuracies = []

for i in range(1,51):
  rf = RandomForestRegressor(random_state = 42, n_estimators = 10, max_depth = i)
  rf.fit(X_train, y_train)

  y_train_pred = rf.predict(X_train)
  y_test_pred = rf.predict(X_test)

  score_train = mean_squared_error(y_train_pred,y_train,squared=False)
  score_train = score_train / (y_train.max() - y_train.min())

  score_test = mean_squared_error(y_test_pred,y_test,squared=False)
  score_test = score_test / (y_test.max() - y_test.min())

  r_rmse_train_accuracies.append(score_train)
  r_rmse_test_accuracies.append(score_test)

  r2_train = r2_score(y_train,y_train_pred)
  r2_test = r2_score(y_test,y_test_pred)

  r_r2_train_accuracies.append(r2_train)
  r_r2_test_accuracies.append(r2_test)

plt.figure(figsize=(4,4))
ax1 = sns.lineplot(data=r_rmse_train_accuracies)
ax2 = sns.lineplot(data=r_rmse_test_accuracies)
plt.xticks(range(1,51))
plt.title('RMSE as Depth Increase')
plt.show()

plt.figure(figsize=(4,4))
ax1 = sns.lineplot(data=r_r2_train_accuracies)
ax2 = sns.lineplot(data=r_r2_test_accuracies)
plt.xticks(range(1,51))
plt.title('R2 as Depth Increases')
plt.show()

"""####ElasticNet"""

from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
for i in range(1,100):
  en = ElasticNet(alpha=1.0, l1_ratio=i/100)
  en.fit(X_train, y_train)
  y_pred = en.predict(X_test)
  lin_reg_score = mean_squared_error(y_test,y_pred,squared=False)
  lin_reg_score = lin_reg_score / (y_test.max() - y_test.min())
  mae = mean_absolute_error(y_test,y_pred)
  made = median_absolute_error(y_test, y_pred)
  r2 = r2_score(y_test,y_pred)
  print(r2)

"""### Neural Network

We'll now move out of the model space of regressors and look at neural networks. We will use a Recurrent Neural Network.

####Recurrent Neural Network
"""

import torch
import torch.nn as nn
import torch.optim as optim

class RNNNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNNNet, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, output_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.4)

    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)

        out, _ = self.rnn(x, h0)
        out = self.dropout(out)
        out = self.fc1(out[:, -1, :])
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)
        return out

input_size = len(X_train.columns)
hidden_size = 64
output_size = 1
learning_rate = 0.0009
num_epochs = 60

model = RNNNet(input_size, hidden_size, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


X1_train = torch.tensor(X_train.values.reshape(-1, 1, input_size), dtype=torch.float32)
Y1_train = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)
X1_test = torch.tensor(X_test.values.reshape(-1, 1, input_size), dtype=torch.float32)
Y1_test = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)

for epoch in range(num_epochs):

    outputs = model(X1_train)

    loss = criterion(outputs, Y1_train)


    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    loss = torch.sqrt(loss)

    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

with torch.no_grad():
    predicted = model(X1_test)
    mse = criterion(predicted, Y1_test)
    mse = torch.sqrt(mse)
    print('MSE on the test set: {:.4f}'.format(mse.item()))

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

scaler = StandardScaler()
scaler.fit(X_train)
scaled_X_train = scaler.transform(X_train)
scaled_X_test = scaler.transform(X_test)

pca = PCA(n_components = 7)
pca_X_train = pca.fit_transform(scaled_X_train)
pca_X_test = pca.transform(scaled_X_test)

explained_variance_ratios = pca.explained_variance_ratio_
components = np.cumsum(pca.explained_variance_ratio_)

components

plt.figure(figsize=(8,8))
plt.plot(np.arange(1,8),components)
plt.title('Proportion of Explained Variance as Number of Components Increases')
plt.xlabel('Number of Components')
plt.ylabel('Proportion of Explained Variance')
plt.show()

reg = LinearRegression().fit(pca_X_train, y_train)
y_pred = reg.predict(pca_X_test)
lin_reg_score = mean_squared_error(y_test,y_pred,squared=False)
lin_reg_score = lin_reg_score / (y_test.max() - y_test.min())
mae = mean_absolute_error(y_test,y_pred)
made = median_absolute_error(y_test, y_pred)
r2 = r2_score(y_test,y_pred)